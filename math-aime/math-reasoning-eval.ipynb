{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grade Solutions to Math Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Qwen2.5-Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clone the Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/QwenLM/Qwen2.5-Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command(command, cwd=None):\n",
    "    \"\"\"\n",
    "    Runs a shell command in the given working directory.\n",
    "    \"\"\"\n",
    "    print(f\"Running: {' '.join(command)} in {cwd or os.getcwd()}\")\n",
    "    subprocess.check_call(command, cwd=cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Change to \"Qwen2.5-Math/evalution/latex2sympy\" and run \"pip install -e .\"\n",
    "    run_command([\"pip\", \"install\", \"-e\", \".\"], cwd=os.path.join(\"Qwen2.5-Math\", \"evaluation\", \"latex2sympy\"))\n",
    "\n",
    "    # Change to the parent directory \"Qwen2.5-Math/evalution\"\n",
    "    evalution_dir = os.path.join(\"Qwen2.5-Math\", \"evaluation\")\n",
    "\n",
    "    # Run \"pip install -r requirements.txt\" in the evalution directory\n",
    "    run_command([\"pip\", \"install\", \"-r\", \"requirements.txt\"], cwd=evalution_dir)\n",
    "\n",
    "    # Install specific versions of vllm and transformers\n",
    "    run_command([\"pip\", \"install\", \"vllm==0.5.1\", \"--no-build-isolation\"], cwd=evalution_dir)\n",
    "    run_command([\"pip\", \"install\", \"transformers\"], cwd=evalution_dir)\n",
    "\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TYPE = \"qwen25-math-cot\"\n",
    "MODEL_NAME_OR_PATH = \"Qwen/Qwen2.5-Math-1.5B-Instruct\"\n",
    "DATA_NAME = \"aime24\"\n",
    "TOKENIZERS_PARALLELISM = False\n",
    "OUTPUT_DIR = \"./results\"\n",
    "SPLIT = \"test\"\n",
    "NUM_TEST_SAMPLE = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "CUDA_VISIBLE_DEVICES = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Eval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_command([\n",
    "    \"python3\", \"-u\", \"math_eval.py\",\n",
    "    \"--model_name_or_path\", MODEL_NAME_OR_PATH,\n",
    "    \"--data_name\", DATA_NAME,\n",
    "    \"--output_dir\", OUTPUT_DIR,\n",
    "    \"--split\", SPLIT,\n",
    "    \"--prompt_type\", PROMPT_TYPE,\n",
    "    \"--num_test_sample\", NUM_TEST_SAMPLE,\n",
    "    \"--seed\", \"0\",\n",
    "    \"--temperature\", \"0\",\n",
    "    \"--n_sampling\", \"1\",\n",
    "    \"--top_p\", \"1\",\n",
    "    \"--start\", \"0\",\n",
    "    \"--end\", \"-1\",\n",
    "    \"--use_vllm\",\n",
    "    \"--save_outputs\",\n",
    "    \"--overwrite\"\n",
    "], cwd=evalution_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(evalution_dir + '/outputs', OUTPUT_DIR + '/aime24')\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_file = [f for f in os.listdir(output_path) if f.endswith('.json')][0]\n",
    "stats_file = os.path.join(output_path, stats_file) # Build the complete file path\n",
    "\n",
    "with open(stats_file, 'r') as f:\n",
    "    stats = json.load(f)\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generated Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = [f for f in os.listdir(output_path) if f.endswith('.jsonl')][0]\n",
    "results_file = os.path.join(output_path, results_file) # Build the complete file path\n",
    "\n",
    "eval_results = []\n",
    "with open(results_file, \"r\") as g:\n",
    "    for line in g:\n",
    "        record = json.loads(line)\n",
    "        eval_results.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Evaluation : o1 grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = '''\n",
    "You are a math teacher tasked to grade a student's solution to a math problem. You will be provided the following data:\n",
    "1. Problem statement.\n",
    "2. The student's solution to the problem.\n",
    "2. Correct answer to the problem.\n",
    "\n",
    "## Instructions: You will grade the student's solution on following aspects:\n",
    "1. Problem Understanding:\n",
    "2. Setup and Strategy:\n",
    "3. Mathematical Execution:\n",
    "4. Correctness\n",
    "\n",
    "For each of the aspects, produce a score between 0 to 4. Provide reasoning for your grading. Provide your grading as a JSON object with the following format: \n",
    "{\"problem_understanding\": <reasoning>..</reasoning><score>4</score>, \"setup_and_strategy\": <reasoning>..</reasoning><score>2</score>, \"mathematical_execution\": <reasoning>..</reasoning><score>1</score>, \"correctness\": <reasoning>..</reasoning><score>0</score>}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add data to the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(prompt, test_example):\n",
    "    prompt += f\"\\n\\nProblem: {test_example['question']}\\n\\nSolution: {test_example['code'][0]}\\n\\nCorrect Answer: {test_example['answer']}\\n\\n\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(build_prompt(SYSTEM_PROMPT, eval_results[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(build_prompt(SYSTEM_PROMPT, eval_results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, test_example in enumerate(eval_results):\n",
    "    print(f\"Processing {i}-th example\")\n",
    "    prompt = build_prompt(SYSTEM_PROMPT, test_example)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"o1-preview\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    test_example[\"grade\"] = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"eval_results_with_grades.json\", \"w\") as f:\n",
    "    json.dump(eval_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['problem_understanding', 'setup_and_strategy', 'mathematical_execution', 'correctness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content(example):\n",
    "    reasonings = [x.split(\"</reasoning>\")[0] for x in example['grade'].split(\"<reasoning>\") if \"</reasoning>\" in x]\n",
    "    scores = [int(x.split(\"</score>\")[0]) for x in example['grade'].split(\"<score>\") if \"</score>\" in x]\n",
    "    score_dict = dict()\n",
    "    for f,score in zip(FEATURES, scores):\n",
    "        score_dict[f] = score\n",
    "    reasoning_dict = dict()\n",
    "    for f,reasoning in zip(FEATURES, reasonings):\n",
    "        reasoning_dict[f] = reasoning\n",
    "    return score_dict, reasoning_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = process_content(eval_results[0])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_scores = {f:0 for f in FEATURES}\n",
    "for i,test_example in enumerate(eval_results):\n",
    "    try:\n",
    "        x,y = process_content(test_example)\n",
    "        for f in FEATURES:\n",
    "            if f == \"correctness\":\n",
    "                x[f] = 1 if x[f] == 4 else 0\n",
    "            aggregate_scores[f] += x[f]\n",
    "    except:\n",
    "        print(f\"Error processing {i}-th example\")\n",
    "\n",
    "aggregate_scores = {f:aggregate_scores[f]/len(eval_results) for f in FEATURES}\n",
    "aggregate_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,val in aggregate_scores.items():\n",
    "    print(f\"{key}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
